{"paragraphs":[{"text":"%md\n#Extractions de features\nVariables nécessaires pour la prediction (on utilise pour l'instant les variables prédictives simples - sans transformation à effectuer) : \n- Pclass, sex ,age,SibSp,Parch,Fare,embarked, survived","authenticationInfo":{},"dateUpdated":"Mar 9, 2016 9:10:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936673_-1095345527","id":"20160309-205536_1770451134","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Extractions de features</h1>\n<p>Variables nécessaires pour la prediction (on utilise pour l'instant les variables prédictives simples - sans transformation à effectuer) :</p>\n<ul>\n<li>Pclass, sex ,age,SibSp,Parch,Fare,embarked, survived</li>\n</ul>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","dateStarted":"Mar 9, 2016 9:10:32 PM","dateFinished":"Mar 9, 2016 9:10:32 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2282"},{"text":"%md\nCas des valeurs incohérentes : \nVu dans le notebook 1, un certain nombre de lignes contienne un age null et fare à 0.\n__Nous prenons le choix pour un premier essai d'exclure les lignes avec age=null & fare=0__","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936673_-1095345527","id":"20160309-205536_895511170","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Cas des valeurs incohérentes :\n<br  />Vu dans le notebook 1, un certain nombre de lignes contienne un age null et fare à 0.\n<br  /><strong>Nous prenons le choix pour un premier essai d'exclure les lignes avec age=null &amp; fare=0</strong></p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2283"},{"title":"Selection des données","text":"val df = sqlContext.sql(\"select Pclass, sex ,age,SibSp,Parch,Fare,embarked , Double(survived) survived from titanic where age is not null and fare!=0 \")\ndf.show(5)\n","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936673_-1095345527","id":"20160309-205536_295889796","result":{"code":"SUCCESS","type":"TEXT","msg":"df: org.apache.spark.sql.DataFrame = [Pclass: int, sex: string, age: double, SibSp: int, Parch: int, Fare: double, embarked: string, survived: double]\n+------+------+----+-----+-----+-------+--------+--------+\n|Pclass|   sex| age|SibSp|Parch|   Fare|embarked|survived|\n+------+------+----+-----+-----+-------+--------+--------+\n|     3|  male|22.0|    1|    0|   7.25|       S|     0.0|\n|     1|female|38.0|    1|    0|71.2833|       C|     1.0|\n|     3|female|26.0|    0|    0|  7.925|       S|     1.0|\n|     1|female|35.0|    1|    0|   53.1|       S|     1.0|\n|     3|  male|35.0|    0|    0|   8.05|       S|     0.0|\n+------+------+----+-----+-----+-------+--------+--------+\nonly showing top 5 rows\n\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2284"},{"title":"API Spark utilisé","text":"%angular\nNous allons utiliser l'API Spark <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">spark.ml</a>","authenticationInfo":{},"dateUpdated":"Mar 9, 2016 9:24:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457558404198_-691476318","id":"20160309-212004_1765786447","result":{"code":"SUCCESS","type":"ANGULAR","msg":"Nous allons utiliser l'API Spark <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\">spark.ml</a>"},"dateCreated":"Mar 9, 2016 9:20:04 PM","dateStarted":"Mar 9, 2016 9:24:03 PM","dateFinished":"Mar 9, 2016 9:24:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2285"},{"text":"%md\n\n###Input /Output algo ML\nLes algorithme ML de spark prennent en entrée __un tableau de Double__ ex : [12.3 23.0 13.0...] ==> les features \nLes algorithme ML de spark prédise un __Double__ ex : 1.0 ==> Survivant","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936674_-1094191280","id":"20160309-205536_1686229077","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Input /Output algo ML</h3>\n<p>Les algorithme ML de spark prennent en entrée <strong>un tableau de Double</strong> ex : [12.3 23.0 13.0&hellip;] ==> les features\n<br  />Les algorithme ML de spark prédise un <strong>Double</strong> ex : 1.0 ==> Survivant</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2286"},{"text":"%md\nNotre datset contient des String (Sex et Embarked) qu'il va falloir transformer en Double\nSurvived est un Int qu'il va faloir convertir en Double","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936674_-1094191280","id":"20160309-205536_987553659","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Notre datset contient des String (Sex et Embarked) qu'il va falloir transformer en Double\n<br  />Survived est un Int qu'il va faloir convertir en Double</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2287"},{"text":"%md\nPour convertir un String en Double utilisation de la fonction Spark native  StringIndexer() qui ajoute un champ Double au DataFrame","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936674_-1094191280","id":"20160309-205536_215914200","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Pour convertir un String en Double utilisation de la fonction Spark native  StringIndexer() qui ajoute un champ Double au DataFrame</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2288"},{"title":"Creation de la variable indexedSex","text":"import org.apache.spark.ml.feature.StringIndexer\n\nval sexIndexer = new StringIndexer()\n  .setInputCol(\"sex\")\n  .setOutputCol(\"indexedSex\")\n  .fit(df)\n  \n val debugDFSexIndexed = sexIndexer.transform(df)//Debug regarder comment est transformer la variable sex\n debugDFSexIndexed.select(\"sex\",\"indexedSex\").show(3)","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936675_-1094576029","id":"20160309-205536_178692620","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.StringIndexer\nsexIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_c74ee81cd27a\ndebugDFSexIndexed: org.apache.spark.sql.DataFrame = [Pclass: int, sex: string, age: double, SibSp: int, Parch: int, Fare: double, embarked: string, survived: double, indexedSex: double]\n+------+----------+\n|   sex|indexedSex|\n+------+----------+\n|  male|       0.0|\n|female|       1.0|\n|female|       1.0|\n+------+----------+\nonly showing top 3 rows\n\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2289"},{"text":"%md\nFaire de même pour la colonne  embarked : la transformer en la variable indexedEmbarked","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936675_-1094576029","id":"20160309-205536_1624230005","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Faire de même pour la colonne  embarked : la transformer en la variable indexedEmbarked</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2290"},{"title":"Création de la variable indexedEmbarked","text":"import org.apache.spark.ml.feature.StringIndexer\n\nval embarkedIndexer = new StringIndexer()\n  .setInputCol(\"embarked\")\n  .setOutputCol(\"indexedEmbarked\")\n  .fit(df)\n  \n  \nval debugDFEmbarkedIndexed = embarkedIndexer.transform(debugDFSexIndexed)\n\ndebugDFEmbarkedIndexed.select(\"embarked\",\"indexedEmbarked\").show(3)","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936675_-1094576029","id":"20160309-205536_1010103312","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.StringIndexer\nembarkedIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_40e31ef237a0\ndebugDFEmbarkedIndexed: org.apache.spark.sql.DataFrame = [Pclass: int, sex: string, age: double, SibSp: int, Parch: int, Fare: double, embarked: string, survived: double, indexedSex: double, indexedEmbarked: double]\n+--------+---------------+\n|embarked|indexedEmbarked|\n+--------+---------------+\n|       S|            0.0|\n|       C|            1.0|\n|       S|            0.0|\n+--------+---------------+\nonly showing top 3 rows\n\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2291"},{"text":"%md\n#### Transfomation des features en tableau de Double ([12.3 24.4....])\nPour cela utilisation de la fonction Spark VectorAssembler","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936676_-1096499773","id":"20160309-205536_1151651096","dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2292"},{"title":"Création de la variable \"features\"","text":"import org.apache.spark.ml.feature.VectorAssembler\n\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"Pclass\",\"indexedSex\" ,\"age\",\"SibSp\",\"Parch\",\"Fare\",\"indexedEmbarked\"))\n  .setOutputCol(\"features\")\n  \n  \n  val debugFeaturesDF =   assembler.transform(debugDFEmbarkedIndexed)\n  \n  debugFeaturesDF.registerTempTable(\"featuresDF\")\n  ","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936676_-1096499773","id":"20160309-205536_2074976578","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.VectorAssembler\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_c2a89af68c92\ndebugFeaturesDF: org.apache.spark.sql.DataFrame = [Pclass: int, sex: string, age: double, SibSp: int, Parch: int, Fare: double, embarked: string, survived: double, indexedSex: double, indexedEmbarked: double, features: vector]\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2293"},{"text":"%sql\nselect * from featuresDF limit 2","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/sql","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936676_-1096499773","id":"20160309-205536_1707033122","result":{"code":"SUCCESS","type":"TABLE","msg":"Pclass\tsex\tage\tSibSp\tParch\tFare\tembarked\tsurvived\tindexedSex\tindexedEmbarked\tfeatures\n3\tmale\t22.0\t1\t0\t7.25\tS\t0.0\t0.0\t0.0\t[3.0,0.0,22.0,1.0,0.0,7.25,0.0]\n1\tfemale\t38.0\t1\t0\t71.2833\tC\t1.0\t1.0\t1.0\t[1.0,1.0,38.0,1.0,0.0,71.2833,1.0]\n","comment":"","msgTable":[[{"key":"sex","value":"3"},{"key":"sex","value":"male"},{"key":"sex","value":"22.0"},{"key":"sex","value":"1"},{"key":"sex","value":"0"},{"key":"sex","value":"7.25"},{"key":"sex","value":"S"},{"key":"sex","value":"0.0"},{"key":"sex","value":"0.0"},{"key":"sex","value":"0.0"},{"key":"sex","value":"[3.0,0.0,22.0,1.0,0.0,7.25,0.0]"}],[{"key":"age","value":"1"},{"key":"age","value":"female"},{"key":"age","value":"38.0"},{"key":"age","value":"1"},{"key":"age","value":"0"},{"key":"age","value":"71.2833"},{"key":"age","value":"C"},{"key":"age","value":"1.0"},{"key":"age","value":"1.0"},{"key":"age","value":"1.0"},{"key":"age","value":"[1.0,1.0,38.0,1.0,0.0,71.2833,1.0]"}]],"columnNames":[{"name":"Pclass","index":0,"aggr":"sum"},{"name":"sex","index":1,"aggr":"sum"},{"name":"age","index":2,"aggr":"sum"},{"name":"SibSp","index":3,"aggr":"sum"},{"name":"Parch","index":4,"aggr":"sum"},{"name":"Fare","index":5,"aggr":"sum"},{"name":"embarked","index":6,"aggr":"sum"},{"name":"survived","index":7,"aggr":"sum"},{"name":"indexedSex","index":8,"aggr":"sum"},{"name":"indexedEmbarked","index":9,"aggr":"sum"},{"name":"features","index":10,"aggr":"sum"}],"rows":[["3","male","22.0","1","0","7.25","S","0.0","0.0","0.0","[3.0,0.0,22.0,1.0,0.0,7.25,0.0]"],["1","female","38.0","1","0","71.2833","C","1.0","1.0","1.0","[1.0,1.0,38.0,1.0,0.0,71.2833,1.0]"]]},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2294"},{"text":"%md\nSpark necessite de tranformer le label avec IndexedLabel pour obtenir des metadonnées sur le label ","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936677_-1096884522","id":"20160309-205536_2125512433","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Spark necessite de tranformer le label avec IndexedLabel pour obtenir des metadonnées sur le label</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2295"},{"text":"val labelIndexer = new StringIndexer()\n  .setInputCol(\"survived\")\n  .setOutputCol(\"survivedIndexed\")\n  .fit(df)","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936677_-1096884522","id":"20160309-205536_480145458","result":{"code":"SUCCESS","type":"TEXT","msg":"labelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_ef1560b36ffb\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2296"},{"text":"%md\nVariable prédictive : features\nVariable cible : survived","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936678_-1095730276","id":"20160309-205536_170882386","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Variable prédictive : features\n<br  />Variable cible : survived</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2297"},{"text":"%md\n#Definition du modèle\nNous prenons le choix de tester une regression logistique","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936678_-1095730276","id":"20160309-205536_362715239","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Definition du modèle</h1>\n<p>Nous prenons le choix de tester une regression logistique</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2298"},{"text":"%angular\n<h2>Logistic regression</h2>\n<img src=\"http://mlpy.sourceforge.net/docs/3.5/_images/elasticnetc.png\"/>\n<img src=\"https://codesachin.files.wordpress.com/2015/08/linearly_separable_4.png\"/>\nRouge : Survivant\n<br/>\nBleu : non survivant\n<br/>\n<b>Notre modèle va permettre de definir cette frontière entre les deux zones </b>","authenticationInfo":{},"dateUpdated":"Mar 9, 2016 9:47:10 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936678_-1095730276","id":"20160309-205536_1847786259","result":{"code":"SUCCESS","type":"ANGULAR","msg":"<h2>Logistic regression</h2>\n<img src=\"http://mlpy.sourceforge.net/docs/3.5/_images/elasticnetc.png\"/>\n<img src=\"https://codesachin.files.wordpress.com/2015/08/linearly_separable_4.png\"/>\nRouge : Survivant\n<br/>\nBleu : non survivant\n<br/>\n<b>Notre modèle va permettre de definir cette frontière entre les deux zones </b>"},"dateCreated":"Mar 9, 2016 8:55:36 PM","dateStarted":"Mar 9, 2016 9:47:10 PM","dateFinished":"Mar 9, 2016 9:47:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2299"},{"text":"%angular\nDocumentation spark du  <a href=\"http://spark.apache.org/docs/1.5.2/api/scala/index.html#org.apache.spark.ml.classification.LogisticRegression\">modèle</a> ","authenticationInfo":{},"dateUpdated":"Mar 9, 2016 9:24:03 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457558560678_1603412954","id":"20160309-212240_2105035483","result":{"code":"SUCCESS","type":"ANGULAR","msg":"Documentation spark du  <a href=\"http://spark.apache.org/docs/1.5.2/api/scala/index.html#org.apache.spark.ml.classification.LogisticRegression\">modèle</a>"},"dateCreated":"Mar 9, 2016 9:22:40 PM","dateStarted":"Mar 9, 2016 9:24:03 PM","dateFinished":"Mar 9, 2016 9:24:03 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2300"},{"text":"import org.apache.spark.ml.classification.LogisticRegression\n\nval model = new LogisticRegression()\n  .setMaxIter(40)\n  .setFeaturesCol(\"features\")\n.setLabelCol(\"survivedIndexed\")\n  ","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936679_-1096115024","id":"20160309-205536_22153477","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.classification.LogisticRegression\nmodel: org.apache.spark.ml.classification.LogisticRegression = logreg_8555dadfd5e4\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2301"},{"text":"%md\n#Definition du Workfow de transformation (Pipeline)\n\nLes étapes de transformation : \n- Transformer sex en Double (via le transformer sexIndexer). sex => indexedSex\n- Transformer embarked en Double (via le transformer embarkedIndexer). embarked=> indexedEmbarked \n- Transformer Pclass,indexedSex ,age,SibSp,Parch,Fare,indexedEmbarked  en tableau de Double (via le transformer assembler) : Pclass,indexedSex ,age,SibSp,Parch,Fare,indexedEmbarked => Features\n- Transformer label en Double (via le transformer labelIndexer). label=> survivedIndexed\n- Estimer le modèle via les variable features et survivedIndexed (via l'estimator model)","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936679_-1096115024","id":"20160309-205536_1881565592","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Definition du Workfow de transformation (Pipeline)</h1>\n<p>Les étapes de transformation :</p>\n<ul>\n<li>Transformer sex en Double (via le transformer sexIndexer). sex => indexedSex</li>\n<li>Transformer embarked en Double (via le transformer embarkedIndexer). embarked=> indexedEmbarked</li>\n<li>Transformer Pclass,indexedSex ,age,SibSp,Parch,Fare,indexedEmbarked  en tableau de Double (via le transformer assembler) : Pclass,indexedSex ,age,SibSp,Parch,Fare,indexedEmbarked => Features</li>\n<li>Transformer label en Double (via le transformer labelIndexer). label=> survivedIndexed</li>\n<li>Estimer le modèle via les variable features et survivedIndexed (via l'estimator model)</li>\n</ul>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2302"},{"text":"import org.apache.spark.ml.Pipeline\n\nval pipeline = new Pipeline()\n  .setStages(Array(sexIndexer,embarkedIndexer,assembler,labelIndexer,  model))","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936679_-1096115024","id":"20160309-205536_286952027","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.Pipeline\npipeline: org.apache.spark.ml.Pipeline = pipeline_1d06ebda2821\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2303"},{"text":"%md \nPasser au notebook 3 : entrainement et analyse du modèle","dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936680_-1098038769","id":"20160309-205536_1754857736","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Passer au notebook 3 : entrainement et analyse du modèle</p>\n"},"dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2304"},{"dateUpdated":"Mar 9, 2016 8:55:36 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1457556936680_-1098038769","id":"20160309-205536_2031232831","dateCreated":"Mar 9, 2016 8:55:36 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2305"}],"name":"02 - Titanic : Construire le modèle","id":"2BEVNMGS9","owners":[],"readers":[],"writers":[],"angularObjects":{"2BDGS26KH":[],"2BFKU3YF6":[],"2BFY4UKAS":[],"2BCYUSJ27":[]},"config":{"looknfeel":"default"},"info":{}}